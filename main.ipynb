{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4244520,"sourceType":"datasetVersion","datasetId":2501389}],"dockerImageVersionId":31234,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aski1140/municipial-waste-management?scriptVersionId=292193228\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nsns.set_style(\"whitegrid\")\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.380972Z","iopub.execute_input":"2026-01-16T11:34:09.381507Z","iopub.status.idle":"2026-01-16T11:34:09.396699Z","shell.execute_reply.started":"2026-01-16T11:34:09.381476Z","shell.execute_reply":"2026-01-16T11:34:09.39557Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Dataset Variable Definitions\n\n| Variable Name | Description |\n|---------------|-------------|\n| **region** | Region |\n| **province** | Province |\n| **name** | Name of municipality |\n| **tc** | Cost per capita |\n| **eur_cres** | Residual cost per capita |\n| **csor** | Sorted cost per capita |\n| **istat** | National code |\n| **area** | Area (kmÂ²) |\n| **pop** | Population |\n| **alt** | Altitude (meters above sea level) |\n| **isle** | Dummy variable for municipality on island |\n| **sea** | Dummy variable for coastal municipality |\n| **pden** | Population density (people per kmÂ²) |\n| **wden** | Waste density (per kmÂ²) |\n| **urb** | Urbanization index (1: low, 3: high) |\n| **fee** | Fee scheme |\n| **d_fee** | Dummy variable for PAYT |\n| **sample** | Region with PAYT |\n| **organic** | Organic waste % |\n| **paper** | Paper waste % |\n| **glass** | Glass waste % |\n| **wood** | Wood waste % |\n| **metal** | Metal waste % |\n| **plastic** | Plastic waste % |\n| **raee** | WEEE (Waste Electrical and Electronic Equipment) % |\n| **texile** | Textile waste % |\n| **other** | Other waste % |\n| **msw_so** | Municipal solid waste sorted (kg) |\n| **msw_un** | Municipal solid waste unsorted (kg) |\n| **msw** | Municipal solid waste (kg) |\n| **sor** | Share of sorted waste |\n| **geo** | Geographic location (1: South, 2: Center, 3: North) |\n| **roads** | Kilometers of roads within the municipality |\n| **s_wteregio** | Share of waste sent to Waste-to-Energy plants (regional figure) |\n| **s_landfill** | Share of waste to landfill |\n| **gdp** | Municipal revenues EUR per capita (log) |\n| **proads** | People per km of roads (log) |\n| **wage** | Taxable income EUR per capita (log) |\n| **finance** | Municipal revenues EUR per capita (log) |\n\n---\n\n## Variable Categories\n\n### ðŸ“ **Geographic Variables**\n- region, province, name, area, alt, isle, sea, geo\n\n### ðŸ‘¥ **Demographic Variables**\n- pop, pden, urb, proads\n\n### â™»ï¸ **Waste Management Variables**\n- organic, paper, glass, wood, metal, plastic, raee, texile, other\n- msw_so, msw_un, msw, sor, wden\n\n### ðŸ’° **Financial Variables**\n- tc, eur_cres, csor, fee, d_fee, gdp, wage, finance\n\n### ðŸ—ï¸ **Infrastructure Variables**\n- roads, s_wteregio, s_landfill","metadata":{}},{"cell_type":"markdown","source":"# CONFIG","metadata":{}},{"cell_type":"code","source":"class config():\n\n    dir_dataset = \"/kaggle/input/municipal-waste-management-cost-prediction/public_data_waste_fee.csv\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.405635Z","iopub.execute_input":"2026-01-16T11:34:09.406176Z","iopub.status.idle":"2026-01-16T11:34:09.424757Z","shell.execute_reply.started":"2026-01-16T11:34:09.406065Z","shell.execute_reply":"2026-01-16T11:34:09.423458Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# IMPORT DATA and EDA","metadata":{}},{"cell_type":"code","source":"# Read csv file\ndf = pd.read_csv(config.dir_dataset)\n\n\n# Select 19 variable from dataset\ninput_features = [\n    \"area\",      # Ã˜â‚€\n    \"pop\",       # Ã˜â‚\n    \"alt\",       # Ã˜â‚‚\n    \"isle\",      # Ã˜â‚ƒ (Island dummy)\n    \"sea\",       # Ã˜â‚„ (Coastal dummy)\n    \"pden\",      # Ã˜â‚…\n    \"wden\",      # Ã˜â‚†\n    \"urb\",       # Ã˜â‚‡\n    \"organic\",   # Ã˜â‚ˆ\n    \"paper\",     # Ã˜â‚‰\n    \"glass\",     # Ã˜â‚â‚€\n    \"wood\",      # Ã˜â‚â‚\n    \"metal\",     # Ã˜â‚â‚‚\n    \"plastic\",   # Ã˜â‚â‚ƒ\n    \"raee\",      # Ã˜â‚â‚„\n    \"texile\",    # Ã˜â‚â‚…\n    \"other\",     # Ã˜â‚â‚†\n    \"msw_so\",    # Ã˜â‚â‚‡\n    \"msw_un\"     # Ã˜â‚â‚ˆ\n]\n\n# Output variable\noutput_feature = \"msw\"  # Ã˜â‚â‚‰\n\nall_features = input_features + [output_feature]\n\ndf = df[all_features].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.426814Z","iopub.execute_input":"2026-01-16T11:34:09.427265Z","iopub.status.idle":"2026-01-16T11:34:09.514579Z","shell.execute_reply.started":"2026-01-16T11:34:09.427238Z","shell.execute_reply":"2026-01-16T11:34:09.513472Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.516465Z","iopub.execute_input":"2026-01-16T11:34:09.516751Z","iopub.status.idle":"2026-01-16T11:34:09.530426Z","shell.execute_reply.started":"2026-01-16T11:34:09.516727Z","shell.execute_reply":"2026-01-16T11:34:09.52904Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.describe().T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.531871Z","iopub.execute_input":"2026-01-16T11:34:09.532353Z","iopub.status.idle":"2026-01-16T11:34:09.593751Z","shell.execute_reply.started":"2026-01-16T11:34:09.532325Z","shell.execute_reply":"2026-01-16T11:34:09.592441Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Probablity Distribution of Features ","metadata":{}},{"cell_type":"code","source":"fig, axes = plt.subplots(nrows = 5, ncols = 4, figsize = (20, 25))\n\naxes = axes.flatten()\n\n\n# Plot histogram for each features\nfor idx, feature in enumerate(all_features):\n    ax = axes[idx]\n    \n    # Histogram + KDE Graph \n    sns.histplot(\n        data=df, \n        x=feature, \n        kde=True,  # Add KDE\n        color=\"#5B9BD5\", \n        edgecolor=\"black\",\n        line_kws={\"color\": \"red\", \"linewidth\": 2},  # KDE color is red \n        stat=\"density\", \n        bins=50,\n        ax=ax\n    )\n    \n    # Set titles and x,y labesl\n    ax.set_title(f\"Histogram of {feature.capitalize()}\", fontsize=12, fontweight=\"bold\")\n    ax.set_xlabel(f\"{feature.capitalize()}\", fontsize=10)\n    ax.set_ylabel(\"Frequency\", fontsize=10)\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:09.596069Z","iopub.execute_input":"2026-01-16T11:34:09.596376Z","iopub.status.idle":"2026-01-16T11:34:14.995981Z","shell.execute_reply.started":"2026-01-16T11:34:09.596351Z","shell.execute_reply":"2026-01-16T11:34:14.994809Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## IMPUTE","metadata":{}},{"cell_type":"markdown","source":"Makalede hangi Impute yÃ¶ntemini kullandÄ±klarÄ±nÄ± sÃ¶ylemediler. Ben kafama gÃ¶re KNN Impute  yÃ¶ntemi uyguladÄ±m","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import KNNImputer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:14.997364Z","iopub.execute_input":"2026-01-16T11:34:14.997789Z","iopub.status.idle":"2026-01-16T11:34:15.003291Z","shell.execute_reply.started":"2026-01-16T11:34:14.99775Z","shell.execute_reply":"2026-01-16T11:34:15.002188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Impute \n\nimputer = KNNImputer(n_neighbors = 5, metric = \"nan_euclidean\", weights = \"distance\")\n\ndf = pd.DataFrame(imputer.fit_transform(df), columns = all_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:15.004539Z","iopub.execute_input":"2026-01-16T11:34:15.004868Z","iopub.status.idle":"2026-01-16T11:34:15.802009Z","shell.execute_reply.started":"2026-01-16T11:34:15.004837Z","shell.execute_reply":"2026-01-16T11:34:15.800903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# MODELING","metadata":{}},{"cell_type":"markdown","source":"Makalede hangi scale yÃ¶ntemini kullandÄ±klarÄ±nÄ± sÃ¶ylemediler. Ben kafama gÃ¶re MinMax Scale yÃ¶ntemi uyguladÄ±m. AyrÄ±ca makalede hangi hiperparametreleri kullandÄ±klarÄ±nÄ± sÃ¶ylemediler bende kendi kafama gÃ¶re uyguladÄ±m.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import  train_test_split, KFold\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers, models, callbacks","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:15.803318Z","iopub.execute_input":"2026-01-16T11:34:15.803592Z","iopub.status.idle":"2026-01-16T11:34:15.809249Z","shell.execute_reply.started":"2026-01-16T11:34:15.803569Z","shell.execute_reply":"2026-01-16T11:34:15.808248Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set random seeds for reproducibility\nnp.random.seed(42)\ntf.random.set_seed(42)","metadata":{"trusted":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2026-01-16T11:34:15.810551Z","iopub.execute_input":"2026-01-16T11:34:15.810984Z","iopub.status.idle":"2026-01-16T11:34:15.858904Z","shell.execute_reply.started":"2026-01-16T11:34:15.810957Z","shell.execute_reply":"2026-01-16T11:34:15.857652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================================\n# 1. DATA PREPARATION\n# ========================================\n\nprint(\"=\"*60)\nprint(\"DATA PREPARATION\")\nprint(\"=\"*60)\n\n# Select input and output features\nX = df[input_features].copy()\ny = df[output_feature].copy()\n\n\n# ========================================\n# 2. DATA SPLITTING\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"DATA SPLITTING\")\nprint(\"=\"*60)\n\n\n# Single split: 3,300 (train) + 1,041 (test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=1041, \n    random_state=45, \n    shuffle=True\n)\n\nprint(f\"\\n Data Split Summary:\")\nprint(f\"   Training Set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"   Test Set:     {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\nprint(f\"   Total:        {len(X)} samples\")\nprint(f\"\\n   â†’ K-Fold CV will be performed on Training Set ({X_train.shape[0]} samples)\")\nprint(f\"   â†’ Final evaluation on Test Set ({X_test.shape[0]} samples)\")\n\n# ========================================\n# 3. NORMALIZATION\n# ========================================\n\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"NORMALIZATION\")\nprint(\"=\"*60)\n\n\nscaler_X = RobustScaler()\nscaler_y = RobustScaler()\n\n# Fit on training data\nX_train_scaled = scaler_X.fit_transform(X_train)\ny_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n\n# Transform test\nX_test_scaled = scaler_X.transform(X_test)\ny_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()\n\nprint(\" Normalization completed!\")\nprint(f\"\\n Scaled Data Statistics:\")\nprint(f\"   X_train: 25%={np.percentile(X_train_scaled, 25):.4f}, median={np.percentile(X_train_scaled, 50):.4f}, 75%={np.percentile(X_train_scaled, 75):.4f}\")\nprint(f\"   y_train: 25%={np.percentile(y_train_scaled, 25):.4f}, median={np.percentile(y_train_scaled, 50):.4f}, 75%={np.percentile(y_train_scaled, 75):.4f}\")\nprint(f\"   X_test:  25%={np.percentile( X_test_scaled, 25):.4f}, median={np.percentile( X_test_scaled, 50):.4f}, 75%={np.percentile( X_test_scaled, 75):.4f}\")\nprint(f\"   y_test:  25%={np.percentile( y_test_scaled, 25):.4f}, median={np.percentile( y_test_scaled, 50):.4f}, 75%={np.percentile( y_test_scaled, 75):.4f}\")\n\n\n# 4. EVALUATION METRICS FUNCTIONS\n# ========================================\n\ndef calculate_metrics(y_true, y_pred, scaler_y, prefix: str = \"Train\"):\n    \"\"\"Calculate ARE, AARE, SD, MSE, RMSE, RÂ² metrics\"\"\"\n\n    \n    print(f\"\\n{'*'*40}\")\n    print(f\"METRICS CALCULATION DEBUG - {prefix}\")\n    print(f\"{'*'*40}\")\n    \n    print(f\"\\n BEFORE Inverse Transform:\")\n    print(f\"   True Y:  25%={np.percentile(y_true, 25):.4f}, median={np.percentile(y_true, 50):.4f}, 75%={np.percentile(y_true, 75):.4f}\")\n    print(f\"   Pred Y:  25%={np.percentile(y_pred, 25):.4f}, median={np.percentile(y_pred, 50):.4f}, 75%={np.percentile(y_pred, 75):.4f}\")\n    \n    # Inverse transform to original scale\n    y_true_original = scaler_y.inverse_transform(y_true.reshape(-1, 1)).ravel()\n    y_pred_original = scaler_y.inverse_transform(y_pred.reshape(-1, 1)).ravel()\n\n\n    print(f\"\\n AFTER Inverse Transform (Original Scale):\")\n    print(f\"   True Y:  25%={np.percentile(y_true_original, 25):.4f}, median={np.percentile(y_true_original, 50):.4f}, 75%={np.percentile(y_true_original, 75):.4f}\")\n    print(f\"   Pred Y:  25%={np.percentile(y_pred_original, 25):.4f}, median={np.percentile(y_pred_original, 50):.4f}, 75%={np.percentile(y_pred_original, 75):.4f}\")\n    \n    print(f\"\\n{'*'*40}\")\n    print(f\"METRICS CALCULATION DEBUG - {prefix} END\")\n    print(f\"{'*'*40}\")\n    \n    \n    # Avoid division by zero\n    y_true_original = np.where(y_true_original == 0, 1e-10, y_true_original)\n    \n    # ARE: Average Relative Error\n    relative_errors = (y_true_original - y_pred_original) / y_true_original\n    ARE = np.mean(relative_errors)\n    \n    # AARE: Average Absolute Relative Error\n    AARE = np.mean(np.abs(relative_errors))\n    \n    # SD: Standard Deviation of errors\n    errors = y_true_original - y_pred_original\n    SD = np.std(errors)\n    \n    # MSE: Mean Squared Error\n    MSE = mean_squared_error(y_true_original, y_pred_original)\n    \n    # RMSE: Root Mean Squared Error\n    RMSE = np.sqrt(MSE)\n    \n    # RÂ²: R-squared\n    R2 = r2_score(y_true_original, y_pred_original)\n    \n    return {\n        f\"{prefix}_ARE\": ARE,\n        f\"{prefix}_AARE\": AARE,\n        f\"{prefix}_SD\": SD,\n        f\"{prefix}_MSE\": MSE,\n        f\"{prefix}_RMSE\": RMSE,\n        f\"{prefix}_R2\": R2\n    }\n\n# ========================================\n# 5. MODEL BUILDING FUNCTIONS\n# ========================================\n\ndef create_cnn_model(input_shape):\n    \"\"\"CNN Model\"\"\"\n    model = models.Sequential([\n        # Input\n        layers.Input(shape=input_shape),\n        \n        # BLOCK 1 - VGG-style (64 filters)\n        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(pool_size=2),\n        \n        # BLOCK 2 - VGG-style (128 filters)\n        layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(pool_size=2),\n        \n        # BLOCK 3 - VGG-style (256 filters)\n        layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv1D(filters=256, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.MaxPooling1D(pool_size=2),\n        \n        # BLOCK 4 - Deeper features (512 filters)\n        layers.Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'),\n        layers.Conv1D(filters=512, kernel_size=3, activation='relu', padding='same'),\n        layers.BatchNormalization(),\n        layers.GlobalAveragePooling1D(),\n        \n        # FULLY CONNECTED LAYERS\n        layers.Dense(512, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n        \n        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n        \n        layers.Dense(128, activation='relu'),\n        \n        layers.Dense(64, activation='relu'),\n        \n        # Output Layer\n        layers.Dense(1, activation='linear')\n    ])\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss=\"mse\",\n        metrics=[\"mae\"]\n    )\n    \n    return model\n\n\ndef create_mlp_model(input_dim):\n    \"\"\"MLP Model\"\"\"\n    model = models.Sequential([\n        layers.Input(shape=(input_dim,)),\n        \n        # Hidden Layer 1 \n        layers.Dense(32, activation='relu'),\n        \n        # Hidden Layer 2\n        layers.Dense(32, activation='relu'),\n        \n        # Output Layer\n        layers.Dense(1, activation='linear')\n    ])\n    \n    model.compile(\n        optimizer=keras.optimizers.Adam(learning_rate=0.001),\n        loss='mse',  # More robust to outliers\n        metrics=['mae']\n    )\n    \n    return model\n\n\ndef create_svm():\n    \"\"\"Create SVM parameters Model\"\"\"\n    \n    model = SVR(\n        kernel='rbf',\n        C=1,\n        gamma='scale',\n        epsilon=0.001,\n        max_iter=10000\n    )\n    \n    return model\n\n\ndef create_lr():\n    \"\"\"Ridge Regression (L2 regularization)\"\"\"\n    \n    model = Ridge(\n        alpha=1.0,\n        solver='auto',\n        random_state=42\n    )\n    \n    return model\n\n# ========================================\n# 6. K-FOLD CROSS VALIDATION FUNCTION\n# ========================================\n\ndef k_fold_cross_validation(X_train_data, \n                            y_train_data, \n                            X_test_data, \n                            y_test_data, \n                            model_type, \n                            k=6, \n                            n_rounds=1, \n                            scaler_y=None, \n                            verbose=1): \n    \"\"\"\n    \n    Parameters:\n    -----------\n    X_train_data: training features\n    y_train_data: training target\n    X_test_data: test features\n    y_test_data: test target\n    model_type: str, one of [\"CNN\", \"MLP\", \"SVM\", \"LR\"]\n    k: int, number of folds (default=6)\n    n_rounds: int, number of rounds (default=1)\n    scaler_y: fitted scaler for inverse transform\n    verbose: int, verbosity level\n    \"\"\"\n    \n    print(f\"\\n{'='*60}\")\n    print(f\"K-FOLD CROSS VALIDATION: {model_type}\")\n    print(f\"k={k} folds, {n_rounds} round(s) â†’ Total {k*n_rounds} validation runs\")\n    print(f\"{'='*60}\")\n    \n    all_results = []\n\n    \n    if model_type == \"CNN\":\n        X_test_cnn = X_test_data.reshape(X_test_data.shape[0], \n                                         X_test_data.shape[1], 1)\n    \n    for round_idx in range(n_rounds):\n        if verbose >= 1:\n            print(f\"\\n{'='*80}\")\n            print(f\"\\n  Round {round_idx + 1}/{n_rounds}\")\n            print(f\"\\n{'='*80}\")\n        \n        # K-Fold split\n        kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n        \n        for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(X_train_data)):\n            if verbose >= 1:\n                print(f\"\\n{'='*60}\")\n                print(f\"   Fold {fold_idx + 1}/{k}...\", end=\" \")\n                print(f\"\\n{'='*60}\")\n           \n            if isinstance(X_train_data, pd.DataFrame):\n                X_train_fold = X_train_data.iloc[train_idx].values\n                X_val_fold = X_train_data.iloc[val_idx].values\n            else:\n                X_train_fold = X_train_data[train_idx]\n                X_val_fold = X_train_data[val_idx]\n            \n            if isinstance(y_train_data, pd.Series):\n                y_train_fold = y_train_data.iloc[train_idx].values\n                y_val_fold = y_train_data.iloc[val_idx].values\n            else:\n                y_train_fold = y_train_data[train_idx]\n                y_val_fold = y_train_data[val_idx]\n            \n            # Train model based on type\n            if model_type == \"CNN\":\n                # Reshape for CNN\n                X_train_cnn = X_train_fold.reshape(X_train_fold.shape[0], \n                                                    X_train_fold.shape[1], 1)\n                X_val_cnn = X_val_fold.reshape(X_val_fold.shape[0], \n                                                X_val_fold.shape[1], 1)\n                \n                \n                \n                model = create_cnn_model((X_train_cnn.shape[1], 1))\n                \n                early_stop = callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    patience=20,\n                    restore_best_weights=True,\n                    verbose=0\n                )\n                \n                reduce_lr = callbacks.ReduceLROnPlateau(\n                    monitor='val_loss',\n                    factor=0.5,\n                    patience=25,\n                    min_lr=1e-7,\n                    verbose=0\n                )\n                \n                model.fit(\n                    X_train_cnn, y_train_fold,\n                    validation_data=(X_val_cnn, y_val_fold),\n                    epochs=500,\n                    batch_size=32,\n                    callbacks=[early_stop, reduce_lr],\n                    verbose=0\n                )\n                y_train_pred = model.predict(X_train_cnn, verbose=0).ravel()\n                y_val_pred = model.predict(X_val_cnn, verbose=0).ravel()\n                y_test_pred = model.predict(X_test_cnn, verbose=0).ravel()\n                \n            elif model_type == \"MLP\":\n                model = create_mlp_model(X_train_fold.shape[1])\n                \n                early_stop = callbacks.EarlyStopping(\n                    monitor='val_loss',\n                    patience=25,\n                    restore_best_weights=True,\n                    verbose=0\n                )\n                \n                reduce_lr = callbacks.ReduceLROnPlateau(\n                    monitor='val_loss',\n                    factor=0.5,\n                    patience=25,\n                    min_lr=1e-7,\n                    verbose=0\n                )\n                \n                model.fit(\n                    X_train_fold, y_train_fold,\n                    validation_data=(X_val_fold, y_val_fold),\n                    epochs=500,\n                    batch_size=32,\n                    callbacks=[early_stop, reduce_lr],\n                    verbose=0 \n                )\n                \n                y_train_pred = model.predict(X_train_fold, verbose=0).ravel()\n                y_val_pred = model.predict(X_val_fold, verbose=0).ravel()\n                y_test_pred = model.predict(X_test_data, verbose=0).ravel()\n                \n            elif model_type == \"SVM\":\n                model = create_svm()\n                model.fit(X_train_fold, y_train_fold)\n                y_train_pred = model.predict(X_train_fold)\n                y_val_pred = model.predict(X_val_fold)\n                y_test_pred = model.predict(X_test_data)\n                \n            elif model_type == \"LR\":\n                model = create_lr()\n                model.fit(X_train_fold, y_train_fold)\n                y_train_pred = model.predict(X_train_fold)\n                y_val_pred = model.predict(X_val_fold)\n                y_test_pred = model.predict(X_test_data)\n            \n            else:\n                raise ValueError(f\"Unknown model_type: {model_type}\")\n            \n            # Calculate metrics for train, val and test set\n\n            metrics_train = calculate_metrics(\n                y_train_fold,      \n                y_train_pred,      \n                scaler_y,         # inverse transform iÃ§in\n                prefix = \"Train\"\n            )\n\n            metrics_val = calculate_metrics(\n                y_val_fold,      \n                y_val_pred,      \n                scaler_y,\n                prefix = \"Val\"        \n            )\n\n            metrics_test = calculate_metrics(\n                y_test_data,      \n                y_test_pred,      \n                scaler_y,\n                prefix = \"Test\"          \n            )\n\n            \n            \n            all_results.append({\n                \"round\": round_idx + 1,\n                \"fold\": fold_idx + 1,\n                **metrics_train,\n                **metrics_val,\n                **metrics_test\n            })\n            \n            if verbose >= 1:\n                print(f\"Metrics in Train: RÂ²={metrics_train['Train_R2']:.4f}, RMSE={metrics_train['Train_RMSE']:.2f}\\n\")\n                print(f\"Metrics in Val:   RÂ²={metrics_val['Val_R2']:.4f}, RMSE={metrics_val['Val_RMSE']:.2f}\\n\")\n                print(f\"Metrics in Test:  RÂ²={metrics_test['Test_R2']:.4f}, RMSE={metrics_test['Test_RMSE']:.2f}\\n\")\n    \n    # Convert to DataFrame\n    results_df = pd.DataFrame(all_results)\n    \n    # Calculate statistics train, val and test set\n\n    # Train Set\n    train_avg_metrics = {\n        \"ARE_mean\": results_df[\"Train_ARE\"].mean(),\n        \"ARE_std\": results_df[\"Train_ARE\"].std(),\n        \"AARE_mean\": results_df[\"Train_AARE\"].mean(),\n        \"AARE_std\": results_df[\"Train_AARE\"].std(),\n        \"SD_mean\": results_df[\"Train_SD\"].mean(),\n        \"SD_std\": results_df[\"Train_SD\"].std(),\n        \"MSE_mean\": results_df[\"Train_MSE\"].mean(),\n        \"MSE_std\": results_df[\"Train_MSE\"].std(),\n        \"RMSE_mean\": results_df[\"Train_RMSE\"].mean(),\n        \"RMSE_std\": results_df[\"Train_RMSE\"].std(),\n        \"R2_mean\": results_df[\"Train_R2\"].mean(),\n        \"R2_std\": results_df[\"Train_R2\"].std(),\n        \"Best_R2\": results_df[\"Train_R2\"].max(),\n        \"Best_RMSE\": results_df[\"Train_RMSE\"].min()\n    }\n    \n    print(f\"\\n  {model_type} K-Fold Completed!\")\n    print(f\"\\n  Average Results ({k*n_rounds} runs for Train Set):\")\n    print(f\"   RÂ² = {train_avg_metrics['R2_mean']:.4f} Â± {train_avg_metrics['R2_std']:.4f}\")\n    print(f\"   RMSE = {train_avg_metrics['RMSE_mean']:.2f} Â± {train_avg_metrics['RMSE_std']:.2f}\")\n    print(f\"   Best RÂ² = {train_avg_metrics['Best_R2']:.4f}\")\n    print(f\"   Best RMSE = {train_avg_metrics['Best_RMSE']:.2f}\")\n\n    print(f\"*\"*60)\n\n    # Val Set\n    val_avg_metrics = {\n        \"ARE_mean\": results_df[\"Val_ARE\"].mean(),\n        \"ARE_std\": results_df[\"Val_ARE\"].std(),\n        \"AARE_mean\": results_df[\"Val_AARE\"].mean(),\n        \"AARE_std\": results_df[\"Val_AARE\"].std(),\n        \"SD_mean\": results_df[\"Val_SD\"].mean(),\n        \"SD_std\": results_df[\"Val_SD\"].std(),\n        \"MSE_mean\": results_df[\"Val_MSE\"].mean(),\n        \"MSE_std\": results_df[\"Val_MSE\"].std(),\n        \"RMSE_mean\": results_df[\"Val_RMSE\"].mean(),\n        \"RMSE_std\": results_df[\"Val_RMSE\"].std(),\n        \"R2_mean\": results_df[\"Val_R2\"].mean(),\n        \"R2_std\": results_df[\"Val_R2\"].std(),\n        \"Best_R2\": results_df[\"Val_R2\"].max(),\n        \"Best_RMSE\": results_df[\"Val_RMSE\"].min()\n    }\n    \n    print(f\"\\n  {model_type} K-Fold Completed!\")\n    print(f\"\\n  Average Results ({k*n_rounds} runs for Val Set):\")\n    print(f\"   RÂ² = {val_avg_metrics['R2_mean']:.4f} Â± {val_avg_metrics['R2_std']:.4f}\")\n    print(f\"   RMSE = {val_avg_metrics['RMSE_mean']:.2f} Â± {val_avg_metrics['RMSE_std']:.2f}\")\n    print(f\"   Best RÂ² = {val_avg_metrics['Best_R2']:.4f}\")\n    print(f\"   Best RMSE = {val_avg_metrics['Best_RMSE']:.2f}\")\n\n    print(f\"*\"*60)\n\n    # Test Set\n\n    test_avg_metrics = {\n        \"ARE_mean\": results_df[\"Test_ARE\"].mean(),\n        \"ARE_std\": results_df[\"Test_ARE\"].std(),\n        \"AARE_mean\": results_df[\"Test_AARE\"].mean(),\n        \"AARE_std\": results_df[\"Test_AARE\"].std(),\n        \"SD_mean\": results_df[\"Test_SD\"].mean(),\n        \"SD_std\": results_df[\"Test_SD\"].std(),\n        \"MSE_mean\": results_df[\"Test_MSE\"].mean(),\n        \"MSE_std\": results_df[\"Test_MSE\"].std(),\n        \"RMSE_mean\": results_df[\"Test_RMSE\"].mean(),\n        \"RMSE_std\": results_df[\"Test_RMSE\"].std(),\n        \"R2_mean\": results_df[\"Test_R2\"].mean(),\n        \"R2_std\": results_df[\"Test_R2\"].std(),\n        \"Best_R2\": results_df[\"Test_R2\"].max(),\n        \"Best_RMSE\": results_df[\"Test_RMSE\"].min()\n    }\n    \n    print(f\"\\n  {model_type} K-Fold Completed!\")\n    print(f\"\\n  Average Results ({k*n_rounds} runs for Test Set):\")\n    print(f\"   RÂ² = {test_avg_metrics['R2_mean']:.4f} Â± {test_avg_metrics['R2_std']:.4f}\")\n    print(f\"   RMSE = {test_avg_metrics['RMSE_mean']:.2f} Â± {test_avg_metrics['RMSE_std']:.2f}\")\n    print(f\"   Best RÂ² = {test_avg_metrics['Best_R2']:.4f}\")\n    print(f\"   Best RMSE = {test_avg_metrics['Best_RMSE']:.2f}\")\n\n    print(f\"*\"*60)\n    \n    return {\n        \"all_results\": results_df,\n        \"train_avg_metrics\": train_avg_metrics,\n        \"val_avg_metrics\": val_avg_metrics,\n        \"test_avg_metrics\": test_avg_metrics,\n        \"model_type\": model_type\n    }\n\ndef get_results_csv_file(results,\n                        models_to_test: dict = {\"SVM\", \"LR\", \"CNN\", \"MLP\"},\n                        looking_feature: str = \"train_avg_metrics\"):\n\n    summary_data = []\n    for model_name in models_to_test:\n        avg = results[model_name][looking_feature]\n        summary_data.append({\n            \"Model\": model_name,\n            \"ARE\": avg[\"ARE_mean\"],\n            \"AARE\": avg[\"AARE_mean\"],\n            \"SD\": avg[\"SD_mean\"],\n            \"MSE\": avg[\"MSE_mean\"],\n            \"RMSE\": avg[\"RMSE_mean\"],\n            \"R2\": avg[\"R2_mean\"]\n        })\n    \n    table_kfold = pd.DataFrame(summary_data)\n    print(\"\\n\" + table_kfold.to_string(index=False))\n\n    # Save results\n    table_kfold.to_csv(f\"{looking_feature}_table_1_kfold_results.csv\", index=False)\n\n    return table_kfold\n\n# ========================================\n# RUN K-FOLD FOR ALL MODELS\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"K-FOLD CROSS VALIDATION (k=6)\")\nprint(\"=\"*60)\n\nmodels_to_test = [\"SVM\", \"LR\",\"MLP\"] #  \"CNN\", \nkfold_results = {}\n\nfor model_name in models_to_test:\n    kfold_results[model_name] = k_fold_cross_validation(\n        X_train_data=X_train_scaled, \n        y_train_data=y_train_scaled,\n        X_test_data=X_test_scaled,\n        y_test_data=y_test_scaled,  \n        model_type=model_name,\n        k=6,\n        n_rounds=1,\n        scaler_y=scaler_y,  \n        verbose=1\n    )\n\n# ========================================\n# 8. K-FOLD SUMMARY TABLE\n# ========================================\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"TABLE 1: K-FOLD CV RESULTS (Train, Val and Test Set)\")\nprint(\"=\"*60)\n\n\n# Print Results and Get CSV File for train, val and test set\ntrain_table = get_results_csv_file(results = kfold_results,\n                                   looking_feature = \"train_avg_metrics\",\n                                   models_to_test  = models_to_test)\n\nval_table = get_results_csv_file(results = kfold_results,\n                                 looking_feature = \"val_avg_metrics\",\n                                 models_to_test  = models_to_test)\n\ntest_table = get_results_csv_file(results = kfold_results,\n                                  looking_feature = \"test_avg_metrics\",\n                                  models_to_test  = models_to_test)\n\n\n\nfor model_name in models_to_test:\n    kfold_results[model_name][\"all_results\"].to_csv(\n        f\"kfold_detailed_{model_name}.csv\", \n        index=False\n    )\n\nprint(\"\\n  K-Fold results saved!\")\nprint(\"\\n\" + \"=\"*60)\nprint(\"K-FOLD CROSS VALIDATION COMPLETED!\")\nprint(\"=\"*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-16T11:40:03.68844Z","iopub.execute_input":"2026-01-16T11:40:03.688856Z","iopub.status.idle":"2026-01-16T11:42:01.890714Z","shell.execute_reply.started":"2026-01-16T11:40:03.688825Z","shell.execute_reply":"2026-01-16T11:42:01.889666Z"},"_kg_hide-input":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}